{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd: Imports the pandas library for data manipulation and analysis, aliased as pd. Pandas is essential for working with DataFrames, which are tabular data structures.\n",
    "\n",
    "import numpy as np: Imports the NumPy library for numerical operations, aliased as np. NumPy provides support for arrays, matrices, and mathematical functions.\n",
    "\n",
    "import matplotlib.pyplot as plt: Imports the pyplot module from the Matplotlib library for creating static, interactive, and animated visualizations in Python, aliased as plt.\n",
    "\n",
    "import seaborn as sns: Imports the Seaborn library for statistical data visualization, aliased as sns. Seaborn is built on top of Matplotlib and provides a high-level interface for creating attractive and informative statistical graphics.\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor: Imports the variance_inflation_factor function from the statsmodels library. This function is used to detect multicollinearity in regression analysis.\n",
    "\n",
    "from ydata_profiling import ProfileReport: Imports the ProfileReport class from the ydata_profiling library. This is used for generating detailed reports on the dataset, including descriptive statistics, data quality issues, and visualizations.\n",
    "\n",
    "from sklearn.compose import ColumnTransformer: Imports ColumnTransformer for applying different preprocessing steps to different columns of the DataFrame.\n",
    "\n",
    "from sklearn.pipeline import Pipeline: Imports Pipeline for creating a sequence of data processing steps.\n",
    "\n",
    "from sklearn.impute import SimpleImputer: Imports SimpleImputer for handling missing values in the dataset (e.g., replacing them with the mean or median).\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder: Imports StandardScaler for standardizing numerical features (mean=0, variance=1) and OneHotEncoder for converting categorical features into a numerical format using one-hot encoding.\n",
    "\n",
    "from sklearn.linear_model import LinearRegression: Imports the LinearRegression model for linear regression analysis.\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor: Imports the RandomForestRegressor model, an ensemble learning method that operates by constructing a multitude of decision trees.\n",
    "\n",
    "from xgboost import XGBRegressor: Imports the XGBRegressor model, an implementation of gradient boosted decision trees designed for speed and performance.\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV: Imports train_test_split for splitting the dataset into training and testing sets, and RandomizedSearchCV for hyperparameter tuning using randomized search with cross-validation.\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score: Imports metrics for evaluating the performance of regression models: Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared (R¬≤).\n",
    "\n",
    "import joblib: Imports the joblib library for saving and loading models.\n",
    "\n",
    "from scipy.stats import randint, uniform: Imports randint for generating random integers and uniform for generating random floating-point numbers, used for hyperparameter tuning.\n",
    "\n",
    "import os: Imports the os module for interacting with the operating system, used here for creating directories.\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid'): Sets the style of Matplotlib plots to 'seaborn-v0_8-whitegrid', which provides a clean, visually appealing style with a white grid.\n",
    "\n",
    "np.random.seed(42): Sets the random seed for NumPy to 42. This ensures reproducibility of results that involve randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\namsi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\core\\decorators.py:262: NumbaDeprecationWarning: \u001b[1mnumba.generated_jit is deprecated. Please see the documentation at: https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-generated-jit for more information and advice on a suitable replacement.\u001b[0m\n",
      "  warnings.warn(msg, NumbaDeprecationWarning)\n",
      "c:\\Users\\namsi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\visions\\backends\\shared\\nan_handling.py:50: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @nb.jit\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from ydata_profiling import ProfileReport\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "from scipy.stats import randint, uniform\n",
    "import os\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Loading & Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try...except block: This is used to handle potential errors when loading the dataset. If the file 'Data.csv' is not found, it prints an error message and exits the script.\n",
    "\n",
    "df = pd.read_csv('Data.csv'): Reads the CSV file named 'Data.csv' into a pandas DataFrame named df.\n",
    "\n",
    "print(\"‚úÖ Dataset loaded successfully\"): Prints a success message if the file is loaded without errors.\n",
    "\n",
    "print(f\"üìä Dataset shape: {df.shape}\\n\"): Prints the shape (number of rows and columns) of the DataFrame.\n",
    "\n",
    "required_columns: Defines a list of column names that are expected to be present in the dataset.\n",
    "\n",
    "missing_columns = set(required_columns) - set(df.columns): Checks if any of the required columns are missing from the DataFrame.\n",
    "\n",
    "if missing_columns:: If there are missing columns, it prints an error message and exits.\n",
    "\n",
    "binary_cols: Defines a list of columns that should contain binary values ('yes' or 'no').\n",
    "\n",
    "for col in binary_cols:: Iterates through each column in binary_cols.\n",
    "\n",
    "invalid = ~df[col].isin(['yes', 'no']): Checks for invalid values (anything other than 'yes' or 'no') in the binary columns.\n",
    "\n",
    "if invalid.any():: If invalid values are found, it prints an error message and exits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset loaded successfully\n",
      "üìä Dataset shape: (545, 13)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = pd.read_csv('Data.csv')\n",
    "    print(\"‚úÖ Dataset loaded successfully\")\n",
    "    print(f\"üìä Dataset shape: {df.shape}\\n\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Error: 'Data.csv' not found in current directory\")\n",
    "    exit()\n",
    "\n",
    "# Validate required columns\n",
    "required_columns = [\n",
    "    'price', 'area', 'bedrooms', 'bathrooms', 'stories',\n",
    "    'mainroad', 'guestroom', 'basement', 'hotwaterheating',\n",
    "    'airconditioning', 'prefarea', 'furnishingstatus'\n",
    "]\n",
    "missing_columns = set(required_columns) - set(df.columns)\n",
    "if missing_columns:\n",
    "    print(f\"‚ùå Missing required columns: {missing_columns}\")\n",
    "    exit()\n",
    "\n",
    "# Validate binary columns\n",
    "binary_cols = [\n",
    "    'mainroad', 'guestroom', 'basement',\n",
    "    'hotwaterheating', 'airconditioning', 'prefarea'\n",
    "]\n",
    "for col in binary_cols:\n",
    "    invalid = ~df[col].isin(['yes', 'no'])\n",
    "    if invalid.any():\n",
    "        print(f\"‚ùå Invalid values in {col}: {df[col][invalid].unique()}\")\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generate EDA Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_eda = df.copy(): Creates a copy of the original DataFrame for EDA to avoid modifying the original data.\n",
    "\n",
    "categorical_cols: Defines a list of categorical columns.\n",
    "\n",
    "for col in categorical_cols:: Converts the specified columns to lowercase strings and then to the 'category' data type.\n",
    "\n",
    "profile = ProfileReport(...): Generates a standard EDA report using ydata_profiling.\n",
    "\n",
    "title: Sets the title of the report.\n",
    "\n",
    "explorative=True: Enables explorative mode for more detailed analysis.\n",
    "\n",
    "config_file=None: Uses default configuration.\n",
    "\n",
    "profile.to_file(\"House_Price_EDA.html\"): Saves the generated report to an HTML file.\n",
    "\n",
    "os.makedirs('eda_plots', exist_ok=True): Creates a directory named 'eda_plots' if it doesn't exist. exist_ok=True prevents an error if the directory already exists.\n",
    "\n",
    "html_content: Initializes an HTML string to store the content for a custom categorical analysis report.\n",
    "\n",
    "create_percent_table(series): Defines a function to create a DataFrame with counts and percentages of each category in a given pandas Series.\n",
    "\n",
    "for col in categorical_cols:: Iterates through each categorical column to create visualizations.\n",
    "\n",
    "Adds HTML content for each column's distribution table and visualization.\n",
    "\n",
    "sns.countplot(): Creates a count plot for binary features, showing the counts of each category.\n",
    "\n",
    "sns.barplot(): Creates a bar plot for multi-class features, showing the counts of each category.\n",
    "\n",
    "plt.savefig(): Saves the generated plot as a PNG image in the 'eda_plots' directory.\n",
    "\n",
    "plt.close(): Closes the current Matplotlib figure to free up memory.\n",
    "\n",
    "with open(\"Categorical_Analysis.html\", \"w\") as f:: Opens a file named \"Categorical_Analysis.html\" in write mode and writes the html_content to it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Generating EDA reports...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e3163cfe9441aca3311e8a0a985779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7747aebd95a4cf2935e381b00eaa056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf1f509b83504dffa8c0ff0284c112ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "659e280533454985a561818b797ff1bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Generating enhanced categorical visualizations...\n",
      "‚úÖ Generated two reports:\n",
      "- House_Price_EDA.html (Standard ydata Profiling)\n",
      "- Categorical_Analysis.html (Custom Visualizations)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüîç Generating EDA reports...\")\n",
    "df_eda = df.copy()\n",
    "\n",
    "# Convert to proper categorical types\n",
    "categorical_cols = ['furnishingstatus'] + binary_cols\n",
    "for col in categorical_cols:\n",
    "    df_eda[col] = df_eda[col].astype(str).str.lower().astype('category')\n",
    "\n",
    "# Generate standard profile report\n",
    "profile = ProfileReport(\n",
    "    df_eda,\n",
    "    title=\"House Price Analysis\",\n",
    "    explorative=True,\n",
    "    config_file=None\n",
    ")\n",
    "profile.to_file(\"House_Price_EDA.html\")\n",
    "\n",
    "# Custom categorical analysis\n",
    "print(\"üìä Generating enhanced categorical visualizations...\")\n",
    "os.makedirs('eda_plots', exist_ok=True)\n",
    "\n",
    "html_content = \"\"\"<html>\n",
    "<head><title>Enhanced Categorical Analysis</title></head>\n",
    "<body style=\"font-family: Arial; padding: 20px;\">\n",
    "<h1>Enhanced Categorical Analysis</h1>\n",
    "\"\"\"\n",
    "\n",
    "def create_percent_table(series):\n",
    "    counts = series.value_counts(dropna=False)\n",
    "    percents = series.value_counts(normalize=True, dropna=False).mul(100).round(1)\n",
    "    return pd.DataFrame({'Count': counts, 'Percentage (%)': percents})\n",
    "\n",
    "for col in categorical_cols:\n",
    "    html_content += f\"<div style='margin: 40px 0; border-top: 2px solid #eee; padding: 20px;'>\"\n",
    "    html_content += f\"<h2>{col.title()}</h2>\"\n",
    "    \n",
    "    # Value counts table\n",
    "    html_content += \"<h3>Distribution</h3>\"\n",
    "    html_content += create_percent_table(df_eda[col]).to_html(classes='data-table', border=0)\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    if df_eda[col].nunique() == 2:  # Binary features\n",
    "        ax = sns.countplot(x=df_eda[col], order=df_eda[col].value_counts().index)\n",
    "        plt.title(f\"{col.title()} Distribution\", fontsize=14)\n",
    "        plt.xlabel('')\n",
    "        \n",
    "        # Add percentages on bars\n",
    "        total = len(df_eda[col])\n",
    "        for p in ax.patches:\n",
    "            percentage = f'{100 * p.get_height()/total:.1f}%'\n",
    "            ax.annotate(percentage, \n",
    "                        (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                        ha='center', va='center', \n",
    "                        xytext=(0, 5), \n",
    "                        textcoords='offset points')\n",
    "    else:  # Multi-class features\n",
    "        plot_data = df_eda[col].value_counts().reset_index()\n",
    "        plot_data.columns = ['Category', 'Count']\n",
    "        \n",
    "        ax = sns.barplot(x='Count', y='Category', data=plot_data, palette='Blues_d')\n",
    "        plt.title(f\"{col.title()} Distribution\", fontsize=14)\n",
    "        plt.xlabel('Count')\n",
    "        plt.ylabel('')\n",
    "        \n",
    "        # Add values on bars\n",
    "        for p in ax.patches:\n",
    "            width = p.get_width()\n",
    "            ax.text(width + 5, p.get_y() + p.get_height()/2,\n",
    "                    f'{int(width)}',\n",
    "                    ha='left', va='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    img_path = f'eda_plots/{col}_distribution.png'\n",
    "    plt.savefig(img_path, bbox_inches='tight', dpi=100)\n",
    "    plt.close()\n",
    "    \n",
    "    html_content += f\"<img src='{img_path}' style='max-width: 800px; margin: 20px 0;'>\"\n",
    "    html_content += \"</div>\"\n",
    "\n",
    "html_content += \"\"\"</body>\n",
    "<style>\n",
    ".data-table {\n",
    "    width: auto !important;\n",
    "    margin: 15px 0;\n",
    "    border-collapse: collapse;\n",
    "}\n",
    ".data-table th, .data-table td {\n",
    "    padding: 8px 12px;\n",
    "    border: 1px solid #ddd;\n",
    "}\n",
    ".data-table th {\n",
    "    background-color: #f8f9fa;\n",
    "}\n",
    "</style>\n",
    "</html>\"\"\"\n",
    "\n",
    "with open(\"Categorical_Analysis.html\", \"w\") as f:\n",
    "    f.write(html_content)\n",
    "\n",
    "print(\"‚úÖ Generated two reports:\")\n",
    "print(\"- House_Price_EDA.html (Standard ydata Profiling)\")\n",
    "print(\"- Categorical_Analysis.html (Custom Visualizations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df[binary_cols] = df[binary_cols].replace({'yes': 1, 'no': 0}).astype(int): Converts the binary columns ('yes'/'no') to numerical (1/0) format.\n",
    "\n",
    "numeric_features: Defines a list of numerical features.\n",
    "\n",
    "categorical_cols: Defines a list of categorical features (only 'furnishingstatus' in this case).\n",
    "\n",
    "preprocessor = ColumnTransformer(...): Creates a ColumnTransformer to apply different preprocessing steps to numerical and categorical features.\n",
    "\n",
    "('num', Pipeline(...), numeric_features): Defines a pipeline for numerical features:\n",
    "\n",
    "SimpleImputer(strategy='median'): Imputes missing values using the median.\n",
    "\n",
    "StandardScaler(): Standardizes numerical features by removing the mean and scaling to unit variance.\n",
    "\n",
    "('cat', OneHotEncoder(...), categorical_cols): Applies one-hot encoding to categorical features.\n",
    "\n",
    "handle_unknown='ignore': Ignores unknown categories during the transformation.\n",
    "\n",
    "drop='first': Drops the first category to avoid multicollinearity (dummy variable trap).\n",
    "\n",
    "remainder='passthrough': Keeps the remaining columns (those not specified in 'num' or 'cat') as they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert binary features to 0/1\n",
    "df[binary_cols] = df[binary_cols].replace({'yes': 1, 'no': 0}).astype(int)\n",
    "\n",
    "numeric_features = ['area', 'bedrooms', 'bathrooms', 'stories']\n",
    "categorical_cols = ['furnishingstatus']\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ]), numeric_features),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore', drop='first'), categorical_cols)\n",
    "], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X = df.drop('price', axis=1): Creates the feature matrix X by dropping the 'price' column (the target variable).\n",
    "\n",
    "y = df['price']: Creates the target variable vector y containing the house prices.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42): Splits the data into training and testing sets using an 80/20 split and a random state for reproducibility.\n",
    "\n",
    "models: Defines a dictionary containing the models to be trained and evaluated. Each model has a name, a model object, and a dictionary of hyperparameters to tune.\n",
    "\n",
    "LinearRegression: A simple linear regression model.\n",
    "\n",
    "RandomForestRegressor: A random forest regressor with hyperparameters for the number of trees (n_estimators), maximum depth of trees (max_depth), and minimum samples required to split a node (min_samples_split).\n",
    "\n",
    "XGBRegressor: An XGBoost regressor with hyperparameters for the number of trees, maximum depth, and learning rate.\n",
    "\n",
    "results: An empty dictionary to store the results of each model.\n",
    "\n",
    "best_model = None: Initializes a variable to store the best model found.\n",
    "\n",
    "best_error_ratio = float('inf'): Initializes a variable to store the best error ratio (initialized to infinity).\n",
    "\n",
    "for name, config in models.items():: Iterates through each model in the models dictionary.\n",
    "\n",
    "pipeline = Pipeline(...): Creates a pipeline that first applies the preprocessor and then trains the specified regressor.\n",
    "\n",
    "search = RandomizedSearchCV(...): Performs a randomized search for hyperparameter tuning using cross-validation.\n",
    "\n",
    "n_iter=20: Specifies the number of parameter settings that are sampled.\n",
    "\n",
    "cv=5: Uses 5-fold cross-validation.\n",
    "\n",
    "scoring='neg_mean_absolute_error' : Uses negative MAE as the scoring metric (lower is better).\n",
    "\n",
    "n_jobs=-1: Uses all available CPU cores for parallel processing.\n",
    "\n",
    "random_state=42: Sets the random seed for reproducibility.\n",
    "\n",
    "search.fit(X_train, y_train): Fits the RandomizedSearchCV object to the training data.\n",
    "\n",
    "best_estimator = search.best_estimator_: Stores the best-performing model found during the search.\n",
    "\n",
    "y_train_pred = best_estimator.predict(X_train): Predicts on the training set using the best model.\n",
    "\n",
    "y_test_pred = best_estimator.predict(X_test): Predicts on the test set using the best model.\n",
    "\n",
    "results[name] = {...}: Stores the best estimator, best parameters, and evaluation metrics for both the training and test sets in the results dictionary.\n",
    "\n",
    "R¬≤: R-squared, a measure of how well the model fits the data (higher is better).\n",
    "\n",
    "MAE: Mean Absolute Error, the average absolute difference between the predicted and actual values (lower is better).\n",
    "\n",
    "RMSE: Root Mean Squared Error, the square root of the average squared difference between the predicted and actual values (lower is better).\n",
    "\n",
    "Error Ratio: The sum of absolute errors divided by the sum of actual values, providing a relative measure of error (lower is better).\n",
    "\n",
    "if results[name]['Test Metrics']['Error Ratio'] < best_error_ratio:: Checks if the current model's error ratio on the test set is lower than the best error ratio found so far.\n",
    "\n",
    "If it is, updates best_error_ratio and best_model.\n",
    "\n",
    "print(\"\\nüìä Final Model Evaluation Metrics:\"): Prints a header for the results.\n",
    "\n",
    "for model, data in results.items():: Iterates through the results of each model and prints the best parameters and evaluation metrics.\n",
    "\n",
    "best_model.fit(X, y): Retrains the best model on the entire dataset (X, y).\n",
    "\n",
    "print(f\"\\nüéØ Best Model: {type(best_model.named_steps['regressor']).__name__}\"): Prints the name of the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è Tuning LinearRegression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\namsi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:307: UserWarning: The total space of parameters 1 is smaller than n_iter=20. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è Tuning RandomForest...\n",
      "\n",
      "‚öôÔ∏è Tuning XGBoost...\n",
      "\n",
      "üìä Final Model Evaluation Metrics:\n",
      "\n",
      "=== LinearRegression ===\n",
      "Best Parameters: {}\n",
      "\n",
      "Training Set:\n",
      "R¬≤: 0.686 | MAE: 719,242.89\n",
      "RMSE: 984,051.92 | Error Ratio: 0.1528\n",
      "\n",
      "Test Set:\n",
      "R¬≤: 0.653 | MAE: 970,043.40\n",
      "RMSE: 1,324,506.96 | Error Ratio: 0.1937\n",
      "\n",
      "=== RandomForest ===\n",
      "Best Parameters: {'regressor__max_depth': 13, 'regressor__min_samples_split': 7, 'regressor__n_estimators': 485}\n",
      "\n",
      "Training Set:\n",
      "R¬≤: 0.880 | MAE: 426,009.21\n",
      "RMSE: 607,401.47 | Error Ratio: 0.0905\n",
      "\n",
      "Test Set:\n",
      "R¬≤: 0.599 | MAE: 1,041,018.30\n",
      "RMSE: 1,423,643.49 | Error Ratio: 0.2079\n",
      "\n",
      "=== XGBoost ===\n",
      "Best Parameters: {'regressor__learning_rate': 0.013979488347959958, 'regressor__max_depth': 3, 'regressor__n_estimators': 415}\n",
      "\n",
      "Training Set:\n",
      "R¬≤: 0.807 | MAE: 555,798.79\n",
      "RMSE: 771,961.38 | Error Ratio: 0.1181\n",
      "\n",
      "Test Set:\n",
      "R¬≤: 0.648 | MAE: 962,949.88\n",
      "RMSE: 1,333,239.67 | Error Ratio: 0.1923\n",
      "\n",
      "üéØ Best Model: XGBRegressor\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('price', axis=1)\n",
    "y = df['price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "models = {\n",
    "    'LinearRegression': {\n",
    "        'model': LinearRegression(),\n",
    "        'params': {}\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'regressor__n_estimators': randint(100, 500),\n",
    "            'regressor__max_depth': randint(2, 20),\n",
    "            'regressor__min_samples_split': randint(2, 20)\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': XGBRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'regressor__n_estimators': randint(100, 500),\n",
    "            'regressor__max_depth': randint(3, 10),\n",
    "            'regressor__learning_rate': uniform(0.01, 0.3)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "results = {}\n",
    "best_model = None\n",
    "best_error_ratio = float('inf')\n",
    "\n",
    "for name, config in models.items():\n",
    "    print(f\"\\n‚öôÔ∏è Tuning {name}...\")\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', config['model'])\n",
    "    ])\n",
    "    \n",
    "    search = RandomizedSearchCV(\n",
    "        pipeline,\n",
    "        config['params'],\n",
    "        n_iter=20,\n",
    "        cv=5,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "    \n",
    "    # Store best estimator and metrics\n",
    "    best_estimator = search.best_estimator_\n",
    "    y_train_pred = best_estimator.predict(X_train)\n",
    "    y_test_pred = best_estimator.predict(X_test)\n",
    "    \n",
    "    results[name] = {\n",
    "        'Best Estimator': best_estimator,\n",
    "        'Best Params': search.best_params_,\n",
    "        'Train Metrics': {\n",
    "            'R¬≤': r2_score(y_train, y_train_pred),\n",
    "            'MAE': mean_absolute_error(y_train, y_train_pred),\n",
    "            'RMSE': np.sqrt(mean_squared_error(y_train, y_train_pred)),\n",
    "            'Error Ratio': np.sum(np.abs(y_train - y_train_pred)) / np.sum(y_train)\n",
    "        },\n",
    "        'Test Metrics': {\n",
    "            'R¬≤': r2_score(y_test, y_test_pred),\n",
    "            'MAE': mean_absolute_error(y_test, y_test_pred),\n",
    "            'RMSE': np.sqrt(mean_squared_error(y_test, y_test_pred)),\n",
    "            'Error Ratio': np.sum(np.abs(y_test - y_test_pred)) / np.sum(y_test)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if results[name]['Test Metrics']['Error Ratio'] < best_error_ratio:\n",
    "        best_error_ratio = results[name]['Test Metrics']['Error Ratio']\n",
    "        best_model = best_estimator\n",
    "\n",
    "# Print results\n",
    "print(\"\\nüìä Final Model Evaluation Metrics:\")\n",
    "for model, data in results.items():\n",
    "    print(f\"\\n=== {model} ===\")\n",
    "    print(f\"Best Parameters: {data['Best Params']}\")\n",
    "    print(\"\\nTraining Set:\")\n",
    "    print(f\"R¬≤: {data['Train Metrics']['R¬≤']:.3f} | MAE: {data['Train Metrics']['MAE']:,.2f}\")\n",
    "    print(f\"RMSE: {data['Train Metrics']['RMSE']:,.2f} | Error Ratio: {data['Train Metrics']['Error Ratio']:.4f}\")\n",
    "    print(\"\\nTest Set:\")\n",
    "    print(f\"R¬≤: {data['Test Metrics']['R¬≤']:.3f} | MAE: {data['Test Metrics']['MAE']:,.2f}\")\n",
    "    print(f\"RMSE: {data['Test Metrics']['RMSE']:,.2f} | Error Ratio: {data['Test Metrics']['Error Ratio']:.4f}\")\n",
    "\n",
    "# Retrain best model on full data\n",
    "best_model.fit(X, y)\n",
    "print(f\"\\nüéØ Best Model: {type(best_model.named_steps['regressor']).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot_feature_importance(model): Defines a function to visualize feature importance.\n",
    "\n",
    "preprocessor = model.named_steps['preprocessor']: Extracts the preprocessor from the trained pipeline.\n",
    "\n",
    "feature_names = preprocessor.get_feature_names_out(): Gets the names of the features after preprocessing.\n",
    "\n",
    "regressor = model.named_steps['regressor']: Extracts the regressor (model) from the pipeline.\n",
    "\n",
    "if hasattr(regressor, 'feature_importances_'):: Checks if the regressor has a feature_importances_ attribute (common in tree-based models).\n",
    "\n",
    "If it does, it gets the feature importances from this attribute.\n",
    "\n",
    "elif hasattr(regressor, 'coef_'):: Checks if the regressor has a coef_ attribute (common in linear models).\n",
    "\n",
    "If it does, it uses the coefficients as a measure of importance.\n",
    "\n",
    "else:: If neither attribute is found, it prints a warning and returns.\n",
    "\n",
    "indices = np.argsort(importances)[-10:]: Gets the indices of the top 10 most important features.\n",
    "\n",
    "plt.figure(figsize=(10, 6)): Creates a Matplotlib figure with a specified size.\n",
    "\n",
    "plt.title('Top 10 Feature Importances'): Sets the title of the plot.\n",
    "\n",
    "plt.barh(...): Creates a horizontal bar plot of the feature importances.\n",
    "\n",
    "clean_names = [name.split('__')[-1] for name in feature_names[indices]]: Cleans the feature names by removing any prefixes added by the preprocessor (e.g., \"num__\" or \"cat__\").\n",
    "\n",
    "plt.yticks(...): Sets the y-axis tick labels to the cleaned feature names.\n",
    "\n",
    "plt.xlabel('Relative Importance'): Sets the x-axis label.\n",
    "\n",
    "plt.tight_layout(): Adjusts the plot layout to prevent labels from overlapping.\n",
    "\n",
    "plt.savefig('feature_importance.png'): Saves the plot as a PNG image.\n",
    "\n",
    "plt.close(): Closes the Matplotlib figure.\n",
    "\n",
    "print(\"‚úÖ Feature importance plot generated successfully\"): Prints a success message.\n",
    "\n",
    "except Exception as e:: Handles any errors during the process and prints an error message.\n",
    "\n",
    "plot_feature_importance(best_model): Calls the function to generate the feature importance plot for the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Feature Importance Analysis\n",
      "‚úÖ Feature importance plot generated successfully\n"
     ]
    }
   ],
   "source": [
    "def plot_feature_importance(model):\n",
    "    \"\"\"Visualize feature importance using model's built-in preprocessor\"\"\"\n",
    "    try:\n",
    "        # Get preprocessor from the trained pipeline\n",
    "        preprocessor = model.named_steps['preprocessor']\n",
    "        feature_names = preprocessor.get_feature_names_out()\n",
    "        \n",
    "        # Get importance values\n",
    "        regressor = model.named_steps['regressor']\n",
    "        if hasattr(regressor, 'feature_importances_'):\n",
    "            importances = regressor.feature_importances_\n",
    "        elif hasattr(regressor, 'coef_'):\n",
    "            importances = regressor.coef_\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Model doesn't support feature importance analysis\")\n",
    "            return\n",
    "\n",
    "        # Create plot with clear labels\n",
    "        indices = np.argsort(importances)[-10:]\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.title('Top 10 Feature Importances')\n",
    "        plt.barh(range(len(indices)), importances[indices], align='center')\n",
    "        \n",
    "        # Clean feature names by removing prefixes\n",
    "        clean_names = [name.split('__')[-1] for name in feature_names[indices]]\n",
    "        plt.yticks(range(len(indices)), clean_names)\n",
    "        plt.xlabel('Relative Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('feature_importance.png')\n",
    "        plt.close()\n",
    "        print(\"‚úÖ Feature importance plot generated successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error generating feature importance: {str(e)}\")\n",
    "\n",
    "# Update the call\n",
    "print(\"\\nüîç Feature Importance Analysis\")\n",
    "plot_feature_importance(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Collinearity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check_collinearity(model): Defines a function to check for multicollinearity among numerical features using Variance Inflation Factor (VIF).\n",
    "\n",
    "preprocessor = model.named_steps['preprocessor']: Extracts the preprocessor from the pipeline.\n",
    "\n",
    "X_processed = preprocessor.transform(X_train): Applies the preprocessing steps to the training data.\n",
    "\n",
    "vif_data = pd.DataFrame(...): Creates a DataFrame to store the VIF values.\n",
    "\n",
    "variance_inflation_factor(...): Calculates the VIF for each numerical feature.\n",
    "\n",
    "X_processed[:, :len(numeric_features)]: Selects only the numerical features from the processed data.\n",
    "\n",
    "for i in range(len(numeric_features)): Iterates through each numerical feature.\n",
    "\n",
    "print(vif_data.to_string(index=False)): Prints the VIF values in a tabular format without the index.\n",
    "\n",
    "except Exception as e:: Handles potential errors during the process and prints an error message.\n",
    "\n",
    "if 'LinearRegression' in results:: Checks if a LinearRegression model was trained.\n",
    "\n",
    "If it was, it calls the check_collinearity function to perform the multicollinearity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Checking multicollinearity for linear model\n",
      "\n",
      "üìä Collinearity Analysis (VIF > 5 indicates high collinearity)\n",
      "  Feature      VIF\n",
      "     area 1.049350\n",
      " bedrooms 1.328012\n",
      "bathrooms 1.253273\n",
      "  stories 1.240142\n"
     ]
    }
   ],
   "source": [
    "def check_collinearity(model):\n",
    "    \"\"\"Simplified collinearity check without tabulate dependency\"\"\"\n",
    "    try:\n",
    "        # Process training data\n",
    "        preprocessor = model.named_steps['preprocessor']\n",
    "        X_processed = preprocessor.transform(X_train)\n",
    "        \n",
    "        # Calculate VIF for numerical features only\n",
    "        vif_data = pd.DataFrame({\n",
    "            'Feature': numeric_features,\n",
    "            'VIF': [variance_inflation_factor(X_processed[:, :len(numeric_features)], i)\n",
    "                   for i in range(len(numeric_features))]\n",
    "        })\n",
    "        \n",
    "        print(\"\\nüìä Collinearity Analysis (VIF > 5 indicates high collinearity)\")\n",
    "        print(vif_data.to_string(index=False))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Collinearity check failed: {str(e)}\")\n",
    "        print(\"Note: Install 'tabulate' for better table formatting: pip install tabulate\")\n",
    "\n",
    "# Update the collinearity check call\n",
    "if 'LinearRegression' in results:\n",
    "    print(\"\\nüîç Checking multicollinearity for linear model\")\n",
    "    check_collinearity(results['LinearRegression']['Best Estimator'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Prediction Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "joblib.dump(best_model, 'best_house_price_model.joblib'): Saves the best-performing model to a file named 'best_house_price_model.joblib' using the joblib library. This allows you to load and reuse the trained model later without having to retrain it.\n",
    "\n",
    "print(\"\\nüíæ Best model saved as best_house_price_model.joblib\"): Prints a message indicating that the model has been saved.\n",
    "\n",
    "validate_input(feature, value): Defines a function to validate and convert user input for each feature.\n",
    "\n",
    "if feature in numeric_features:: Checks if the feature is numerical.\n",
    "\n",
    "Converts the input value to a float.\n",
    "\n",
    "Performs specific validation based on the feature (e.g., bedrooms must be between 0 and 10).\n",
    "\n",
    "Raises a ValueError if the input is invalid.\n",
    "\n",
    "if feature in binary_cols:: Checks if the feature is binary.\n",
    "\n",
    "Converts the input to lowercase and removes leading/trailing whitespace.\n",
    "\n",
    "Converts 'yes'/'1' to 1 and 'no'/'0' to 0.\n",
    "\n",
    "Raises a ValueError if the input is not 'yes', 'no', '1', or '0'.\n",
    "\n",
    "if feature == 'furnishingstatus':: Checks if the feature is 'furnishingstatus'.\n",
    "\n",
    "Validates that the input is one of the allowed values ('furnished', 'semi-furnished', 'unfurnished').\n",
    "\n",
    "Raises a ValueError if the input is invalid.\n",
    "\n",
    "return value: Returns the validated and converted value.\n",
    "\n",
    "except ValueError as e:: Handles ValueError exceptions raised during validation and prints an error message.\n",
    "\n",
    "predict_price(): Defines a function to provide a user interface for predicting house prices.\n",
    "\n",
    "print(\"\\nüè† Property Price Prediction Interface\"): Prints a header for the interface.\n",
    "\n",
    "feature_values = {}: Initializes an empty dictionary to store the feature values entered by the user.\n",
    "\n",
    "for feature in X.columns:: Iterates through each feature in the feature matrix X.\n",
    "\n",
    "while True:: Enters a loop that continues until valid input is received.\n",
    "\n",
    "value = input(f\"{feature}: \"): Prompts the user to enter a value for the current feature.\n",
    "\n",
    "validated = validate_input(feature, value): Calls the validate_input function to validate and convert the input.\n",
    "\n",
    "feature_values[feature] = [validated]: Stores the validated value in the feature_values dictionary.\n",
    "\n",
    "break: Exits the loop if the input is valid.\n",
    "\n",
    "except ValueError as e:: Handles ValueError exceptions raised during validation and prints an error message.\n",
    "\n",
    "try...except block: Handles potential errors during model loading and prediction.\n",
    "\n",
    "model = joblib.load('best_house_price_model.joblib'): Loads the saved best model from the file.\n",
    "\n",
    "prediction = model.predict(pd.DataFrame(feature_values))[0]: Creates a DataFrame from the user input and uses the loaded model to predict the house price.\n",
    "\n",
    "print(f\"\\nüíµ Predicted Price: ‚Çπ{prediction:,.2f}\"): Prints the predicted price, formatted with commas as thousands separators.\n",
    "\n",
    "except Exception as e:: Handles any exceptions during the process and prints an error message.\n",
    "\n",
    "print(\"\\nüöÄ Production-Ready Price Prediction System!\"): Prints a final message indicating that the system is ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Best model saved as best_house_price_model.joblib\n",
      "\n",
      "üè† Property Price Prediction Interface\n",
      "‚ùå Error: Invalid furnishingstatus: Must be one of ['furnished', 'semi-furnished', 'unfurnished']\n",
      "\n",
      "üíµ Predicted Price: $3,028,940.75\n",
      "\n",
      "üöÄ Production-Ready Price Prediction System!\n"
     ]
    }
   ],
   "source": [
    "joblib.dump(best_model, 'best_house_price_model.joblib')\n",
    "print(\"\\nüíæ Best model saved as best_house_price_model.joblib\")\n",
    "\n",
    "def validate_input(feature, value):\n",
    "    \"\"\"Validate and convert input features\"\"\"\n",
    "    try:\n",
    "        if feature in numeric_features:\n",
    "            val = float(value)\n",
    "            if feature == 'bedrooms' and not 0 <= val <= 10:\n",
    "                raise ValueError(\"Bedrooms must be 0-10\")\n",
    "            if feature == 'bathrooms' and val < 0:\n",
    "                raise ValueError(\"Bathrooms can't be negative\")\n",
    "            if feature == 'area' and val < 100:\n",
    "                raise ValueError(\"Area must be ‚â•100 sqft\")\n",
    "            return val\n",
    "        \n",
    "        if feature in binary_cols:\n",
    "            clean_val = str(value).lower().strip()\n",
    "            if clean_val in ['yes', '1']: return 1\n",
    "            if clean_val in ['no', '0']: return 0\n",
    "            raise ValueError(f\"Must be yes/no or 0/1 for {feature}\")\n",
    "        \n",
    "        if feature == 'furnishingstatus':\n",
    "            valid = ['furnished', 'semi-furnished', 'unfurnished']\n",
    "            if value.lower() not in valid:\n",
    "                raise ValueError(f\"Must be one of {valid}\")\n",
    "            return value\n",
    "        \n",
    "        return value\n",
    "    except ValueError as e:\n",
    "        raise ValueError(f\"Invalid {feature}: {str(e)}\")\n",
    "\n",
    "def predict_price():\n",
    "    \"\"\"Safe prediction interface\"\"\"\n",
    "    print(\"\\nüè† Property Price Prediction Interface\")\n",
    "    feature_values = {}\n",
    "    \n",
    "    for feature in X.columns:\n",
    "        while True:\n",
    "            try:\n",
    "                value = input(f\"{feature}: \")\n",
    "                validated = validate_input(feature, value)\n",
    "                feature_values[feature] = [validated]\n",
    "                break\n",
    "            except ValueError as e:\n",
    "                print(f\"‚ùå Error: {str(e)}\")\n",
    "    \n",
    "    try:\n",
    "        model = joblib.load('best_house_price_model.joblib')\n",
    "        prediction = model.predict(pd.DataFrame(feature_values))[0]\n",
    "        print(f\"\\nüíµ Predicted Price: ${prediction:,.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Prediction failed: {str(e)}\")\n",
    "\n",
    "# Uncomment to enable interactive predictions\n",
    "predict_price()\n",
    "\n",
    "print(\"\\nüöÄ Production-Ready Price Prediction System!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
